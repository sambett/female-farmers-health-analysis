{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate the imputation results with improved visualizations\n",
    "def validate_imputation(original_data, imputed_data, key_columns):\n",
    "    \"\"\"\n",
    "    Validate the imputation results by comparing distributions\n",
    "    Only creates visualizations for variables that actually had missing values\n",
    "    \"\"\"\n",
    "    print(\"\\nValidating imputation results...\")\n",
    "    \n",
    "    # Filter to only include columns that had missing values\n",
    "    columns_with_missing = [col for col in key_columns \n",
    "                           if col in original_data.columns \n",
    "                           and col in imputed_data.columns\n",
    "                           and original_data[col].isnull().sum() > 0]\n",
    "    \n",
    "    if not columns_with_missing:\n",
    "        print(\"  No columns with missing values to validate.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"  Validating {len(columns_with_missing)} columns with missing values\")\n",
    "    \n",
    "    # Organize columns by missingness percentage\n",
    "    missing_pcts = {col: original_data[col].isnull().mean() * 100 for col in columns_with_missing}\n",
    "    \n",
    "    # Group columns by missing percentage\n",
    "    high_missing = [col for col, pct in missing_pcts.items() if pct >= 30]\n",
    "    mod_missing = [col for col, pct in missing_pcts.items() if 10 <= pct < 30]\n",
    "    low_missing = [col for col, pct in missing_pcts.items() if pct < 10]\n",
    "    \n",
    "    # Create separate figures for each group to avoid overcrowding\n",
    "    plot_groups = []\n",
    "    if high_missing:\n",
    "        plot_groups.append((\"High Missingness Variables (≥30%)\", high_missing))\n",
    "    if mod_missing:\n",
    "        plot_groups.append((\"Moderate Missingness Variables (10-30%)\", mod_missing))\n",
    "    if low_missing:\n",
    "        plot_groups.append((\"Low Missingness Variables (<10%)\", low_missing))\n",
    "    \n",
    "    # Process each group\n",
    "    for group_title, cols in plot_groups:\n",
    "        # Setup plotting for this group\n",
    "        n_cols = len(cols)\n",
    "        n_rows = (n_cols + 1) // 2  # Ceiling division by 2\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(n_rows, 2, figsize=(16, 6 * n_rows))\n",
    "        fig.suptitle(group_title, fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Flatten axes for easy indexing\n",
    "        if n_rows > 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes[0], axes[1]] if isinstance(axes, np.ndarray) else [axes]  # Handle single row case\n",
    "        \n",
    "        # Generate comparison plots for each column\n",
    "        for i, col in enumerate(cols):\n",
    "            if i < len(axes):\n",
    "                ax = axes[i]\n",
    "                \n",
    "                # Get missing percentage for title\n",
    "                missing_pct = missing_pcts[col]\n",
    "                \n",
    "                # Get original data (non-missing), originally missing values, and all imputed data\n",
    "                orig_data = original_data[col].dropna()\n",
    "                missing_mask = original_data[col].isnull()\n",
    "                imputed_missing = imputed_data.loc[missing_mask, col]\n",
    "                all_imputed = imputed_data[col]\n",
    "                \n",
    "                # Set title with missing percentage\n",
    "                ax.set_title(f\"{col} ({missing_pct:.1f}% imputed)\", fontsize=12, fontweight='bold')\n",
    "                \n",
    "                # Check if numeric or categorical\n",
    "                if pd.api.types.is_numeric_dtype(orig_data):\n",
    "                    # For numeric variables, create a more informative distribution plot\n",
    "                    # Use kernel density estimation for smoother visualization\n",
    "                    sns.kdeplot(orig_data, ax=ax, color='#1f77b4', label='Original', linewidth=2.5)\n",
    "                    sns.kdeplot(all_imputed, ax=ax, color='#ff7f0e', label='After Imputation', linewidth=2)\n",
    "                    \n",
    "                    # Add a histogram of just the imputed values to see their distribution\n",
    "                    if len(imputed_missing) > 0:\n",
    "                        sns.histplot(imputed_missing, ax=ax, color='#d62728', alpha=0.4, \n",
    "                                    label='Imputed Values Only', kde=False)\n",
    "                    \n",
    "                    # Add vertical lines for means\n",
    "                    ax.axvline(orig_data.mean(), color='#1f77b4', linestyle='--', linewidth=1.5)\n",
    "                    ax.axvline(all_imputed.mean(), color='#ff7f0e', linestyle='--', linewidth=1.5)\n",
    "                    \n",
    "                    # Add statistics as an inset text box\n",
    "                    orig_stats = f\"Original (n={len(orig_data)}): mean={orig_data.mean():.2f}, std={orig_data.std():.2f}\"\n",
    "                    imp_stats = f\"After Imputation (n={len(all_imputed)}): mean={all_imputed.mean():.2f}, std={all_imputed.std():.2f}\"\n",
    "                    imp_only_stats = f\"Imputed Only (n={len(imputed_missing)}): mean={imputed_missing.mean():.2f}, std={imputed_missing.std():.2f}\"\n",
    "                    \n",
    "                    stats_text = f\"{orig_stats}\\n{imp_stats}\\n{imp_only_stats}\"\n",
    "                    \n",
    "                    # Calculate and display KS test p-value if enough data\n",
    "                    if len(orig_data) >= 5 and len(all_imputed) >= 5:\n",
    "                        from scipy.stats import ks_2samp\n",
    "                        ks_stat, p_val = ks_2samp(orig_data, all_imputed)\n",
    "                        stats_text += f\"\\nKS test p-value: {p_val:.3f}\"\n",
    "                        \n",
    "                        # Add a qualitative assessment\n",
    "                        if p_val > 0.05:\n",
    "                            stats_text += \" (distributions not significantly different)\"\n",
    "                        else:\n",
    "                            stats_text += \" (distributions significantly different)\"\n",
    "                    \n",
    "                    # Add text box\n",
    "                    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, va='top', \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=9)\n",
    "                    \n",
    "                    # Print mean and std differences for reference\n",
    "                    mean_diff = abs(orig_data.mean() - all_imputed.mean())\n",
    "                    std_diff = abs(orig_data.std() - all_imputed.std())\n",
    "                    \n",
    "                    # Calculate mean difference as percentage of original std (useful metric)\n",
    "                    if orig_data.std() > 0:\n",
    "                        std_pct_diff = (mean_diff / orig_data.std()) * 100\n",
    "                        print(f\"  {col}: mean diff = {mean_diff:.2f} ({std_pct_diff:.1f}% of original std)\")\n",
    "                    else:\n",
    "                        print(f\"  {col}: mean diff = {mean_diff:.2f}\")\n",
    "                    \n",
    "                else:\n",
    "                    # For categorical variables, create a grouped bar chart\n",
    "                    # Create value counts for visualization\n",
    "                    orig_counts = orig_data.value_counts(normalize=True).sort_index()\n",
    "                    all_imp_counts = all_imputed.value_counts(normalize=True).sort_index()\n",
    "                    \n",
    "                    if len(imputed_missing) > 0:\n",
    "                        imp_only_counts = imputed_missing.value_counts(normalize=True).sort_index()\n",
    "                    else:\n",
    "                        imp_only_counts = pd.Series(dtype='float64')\n",
    "                    \n",
    "                    # Combine all unique categories\n",
    "                    all_cats = sorted(set(list(orig_counts.index) + \n",
    "                                       list(all_imp_counts.index) + \n",
    "                                       list(imp_only_counts.index)))\n",
    "                    \n",
    "                    # Create a DataFrame for plotting with all three distributions\n",
    "                    plot_data = []\n",
    "                    \n",
    "                    # Add original distribution\n",
    "                    for cat in all_cats:\n",
    "                        value = orig_counts.get(cat, 0)\n",
    "                        plot_data.append({\n",
    "                            'Category': cat,\n",
    "                            'Proportion': value,\n",
    "                            'Source': 'Original'\n",
    "                        })\n",
    "                    \n",
    "                    # Add after imputation distribution\n",
    "                    for cat in all_cats:\n",
    "                        value = all_imp_counts.get(cat, 0)\n",
    "                        plot_data.append({\n",
    "                            'Category': cat,\n",
    "                            'Proportion': value,\n",
    "                            'Source': 'After Imputation'\n",
    "                        })\n",
    "                    \n",
    "                    # Add imputed-only distribution if we have imputed values\n",
    "                    if len(imputed_missing) > 0:\n",
    "                        for cat in all_cats:\n",
    "                            value = imp_only_counts.get(cat, 0)\n",
    "                            plot_data.append({\n",
    "                                'Category': cat,\n",
    "                                'Proportion': value,\n",
    "                                'Source': 'Imputed Only'\n",
    "                            })\n",
    "                    \n",
    "                    # Convert to DataFrame\n",
    "                    plot_df = pd.DataFrame(plot_data)\n",
    "                    \n",
    "                    # Plot bar chart\n",
    "                    sns.barplot(data=plot_df, x='Category', y='Proportion', hue='Source', \n",
    "                               palette=['#1f77b4', '#ff7f0e', '#d62728'], ax=ax)\n",
    "                    \n",
    "                    # Customize x-axis labels\n",
    "                    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                    \n",
    "                    # Add counts as text\n",
    "                    stats_text = f\"Original n={len(orig_data)}, After Imputation n={len(all_imputed)}, Imputed Only n={len(imputed_missing)}\"\n",
    "                    \n",
    "                    # Calculate chi-square test for distribution comparison\n",
    "                    try:\n",
    "                        from scipy.stats import chi2_contingency\n",
    "                        \n",
    "                        # Create contingency table\n",
    "                        table = pd.DataFrame({\n",
    "                            'Original': [orig_counts.get(cat, 0) * len(orig_data) for cat in all_cats],\n",
    "                            'Imputed': [all_imp_counts.get(cat, 0) * len(all_imputed) for cat in all_cats]\n",
    "                        })\n",
    "                        \n",
    "                        # Chi-square test\n",
    "                        chi2, p_val, _, _ = chi2_contingency(table)\n",
    "                        stats_text += f\"\\nChi² test p-value: {p_val:.3f}\"\n",
    "                        \n",
    "                        # Add qualitative assessment\n",
    "                        if p_val > 0.05:\n",
    "                            stats_text += \" (distributions not significantly different)\"\n",
    "                        else:\n",
    "                            stats_text += \" (distributions significantly different)\"\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        # If chi-square test fails, use Hellinger distance instead\n",
    "                        hell_dist = np.sqrt(np.sum((np.sqrt(orig_counts.reindex(all_cats).fillna(0)) - \n",
    "                                               np.sqrt(all_imp_counts.reindex(all_cats).fillna(0))) ** 2)) / np.sqrt(2)\n",
    "                        stats_text += f\"\\nHellinger distance: {hell_dist:.3f}\"\n",
    "                        \n",
    "                        # Add qualitative assessment\n",
    "                        if hell_dist < 0.2:\n",
    "                            stats_text += \" (distributions very similar)\"\n",
    "                        elif hell_dist < 0.4:\n",
    "                            stats_text += \" (distributions moderately similar)\"\n",
    "                        else:\n",
    "                            stats_text += \" (distributions differ substantially)\"\n",
    "                    \n",
    "                    # Add text box\n",
    "                    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, va='top', \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=9)\n",
    "                \n",
    "                # Consistent legend with better placement\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                ax.legend(handles, labels, loc='best', frameon=True, fontsize=9, \n",
    "                         title=\"Data Source\")\n",
    "            \n",
    "        # Remove any unused subplots\n",
    "        for i in range(len(cols), len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "            \n",
    "        # Improve spacing\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "        \n",
    "        # Save figure\n",
    "        group_name = group_title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('%', 'pct')\n",
    "        plt.savefig(f'imputation_validation_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  Validation plots for {group_title} saved to 'imputation_validation_{group_name}.png'\")\n",
    "    \n",
    "    # Create a summary table of imputation quality metrics\n",
    "    print(\"\\nCreating imputation quality summary table...\")\n",
    "    \n",
    "    # Collect metrics for each variable\n",
    "    metrics = []\n",
    "    for col in columns_with_missing:\n",
    "        orig_data = original_data[col].dropna()\n",
    "        missing_mask = original_data[col].isnull()\n",
    "        imputed_missing = imputed_data.loc[missing_mask, col]\n",
    "        all_imputed = imputed_data[col]\n",
    "        \n",
    "        metric = {\n",
    "            'Variable': col,\n",
    "            'Missing %': missing_pcts[col],\n",
    "            'Missing Count': missing_mask.sum(),\n",
    "            'Total Count': len(original_data)\n",
    "        }\n",
    "        \n",
    "        # Calculate appropriate metrics based on variable type\n",
    "        if pd.api.types.is_numeric_dtype(orig_data):\n",
    "            # For numeric variables\n",
    "            metric['Original Mean'] = orig_data.mean()\n",
    "            metric['Original Std'] = orig_data.std()\n",
    "            metric['Imputed Mean'] = all_imputed.mean()\n",
    "            metric['Imputed Std'] = all_imputed.std()\n",
    "            metric['Mean Diff'] = abs(metric['Original Mean'] - metric['Imputed Mean'])\n",
    "            \n",
    "            if metric['Original Std'] > 0:\n",
    "                metric['Mean Diff (% of Std)'] = (metric['Mean Diff'] / metric['Original Std']) * 100\n",
    "            else:\n",
    "                metric['Mean Diff (% of Std)'] = None\n",
    "                \n",
    "            # Calculate KS test if enough data\n",
    "            if len(orig_data) >= 5 and len(all_imputed) >= 5:\n",
    "                from scipy.stats import ks_2samp\n",
    "                ks_stat, p_val = ks_2samp(orig_data, all_imputed)\n",
    "                metric['KS Test p-value'] = p_val\n",
    "                \n",
    "                # Determine quality\n",
    "                if p_val > 0.05:\n",
    "                    if metric['Mean Diff (% of Std)'] is not None and metric['Mean Diff (% of Std)'] < 20:\n",
    "                        metric['Quality'] = 'Good'\n",
    "                    else:\n",
    "                        metric['Quality'] = 'Acceptable'\n",
    "                else:\n",
    "                    if missing_pcts[col] > 50:\n",
    "                        metric['Quality'] = 'Acceptable (high missingness)'\n",
    "                    else:\n",
    "                        metric['Quality'] = 'Review'\n",
    "            else:\n",
    "                metric['KS Test p-value'] = None\n",
    "                metric['Quality'] = 'Insufficient Data'\n",
    "                \n",
    "        else:\n",
    "            # For categorical variables\n",
    "            orig_counts = orig_data.value_counts(normalize=True)\n",
    "            all_imp_counts = all_imputed.value_counts(normalize=True)\n",
    "            \n",
    "            # Most frequent category\n",
    "            metric['Original Mode'] = orig_data.mode().iloc[0] if len(orig_data) > 0 else None\n",
    "            metric['Imputed Mode'] = all_imputed.mode().iloc[0]\n",
    "            metric['Mode Changed'] = metric['Original Mode'] != metric['Imputed Mode']\n",
    "            \n",
    "            # Calculate chi-square or Hellinger distance\n",
    "            try:\n",
    "                from scipy.stats import chi2_contingency\n",
    "                all_cats = sorted(set(list(orig_counts.index) + list(all_imp_counts.index)))\n",
    "                \n",
    "                # Create contingency table\n",
    "                table = pd.DataFrame({\n",
    "                    'Original': [orig_counts.get(cat, 0) * len(orig_data) for cat in all_cats],\n",
    "                    'Imputed': [all_imp_counts.get(cat, 0) * len(all_imputed) for cat in all_cats]\n",
    "                })\n",
    "                \n",
    "                # Chi-square test\n",
    "                chi2, p_val, _, _ = chi2_contingency(table)\n",
    "                metric['Chi² Test p-value'] = p_val\n",
    "                \n",
    "                # Determine quality\n",
    "                if p_val > 0.05:\n",
    "                    metric['Quality'] = 'Good'\n",
    "                else:\n",
    "                    if missing_pcts[col] > 50:\n",
    "                        metric['Quality'] = 'Acceptable (high missingness)'\n",
    "                    else:\n",
    "                        metric['Quality'] = 'Review'\n",
    "                        \n",
    "            except Exception:\n",
    "                # Fall back to Hellinger distance\n",
    "                all_cats = sorted(set(list(orig_counts.index) + list(all_imp_counts.index)))\n",
    "                hell_dist = np.sqrt(np.sum((np.sqrt(orig_counts.reindex(all_cats).fillna(0)) - \n",
    "                                       np.sqrt(all_imp_counts.reindex(all_cats).fillna(0))) ** 2)) / np.sqrt(2)\n",
    "                metric['Hellinger Distance'] = hell_dist\n",
    "                \n",
    "                # Determine quality\n",
    "                if hell_dist < 0.2:\n",
    "                    metric['Quality'] = 'Good'\n",
    "                elif hell_dist < 0.4:\n",
    "                    metric['Quality'] = 'Acceptable'\n",
    "                else:\n",
    "                    metric['Quality'] = 'Review'\n",
    "        \n",
    "        metrics.append(metric)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    metrics_df = metrics_df.sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    # Save as CSV\n",
    "    metrics_df.to_csv('imputation_quality_metrics.csv', index=False)\n",
    "    print(f\"  Imputation quality metrics saved to 'imputation_quality_metrics.csv'\")\n",
    "    \n",
    "    # Create a visualization of the quality metrics\n",
    "    plt.figure(figsize=(14, len(metrics_df) * 0.8 + 2))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Define colors for quality\n",
    "    quality_colors = {\n",
    "        'Good': '#1b9e77',           # Green\n",
    "        'Acceptable': '#7570b3',      # Purple\n",
    "        'Acceptable (high missingness)': '#7570b3', # Same purple\n",
    "        'Review': '#d95f02',          # Orange\n",
    "        'Insufficient Data': '#999999' # Gray\n",
    "    }\n",
    "    \n",
    "    # Format the DataFrame for display\n",
    "    display_df = metrics_df[['Variable', 'Missing %', 'Quality']].copy()\n",
    "    display_df['Missing %'] = display_df['Missing %'].round(1).astype(str) + '%'\n",
    "    \n",
    "    # Create the table\n",
    "    table = ax.table(\n",
    "        cellText=display_df.values,\n",
    "        colLabels=display_df.columns,\n",
    "        loc='center',\n",
    "        cellLoc='center',\n",
    "        cellColours=[[1, 1, 1, 0] for _ in range(len(display_df) * 3)]  # Default white\n",
    "    )\n",
    "    \n",
    "    # Manually set color for quality cells\n",
    "    for i, quality in enumerate(metrics_df['Quality']):\n",
    "        table[(i+1, 2)].set_facecolor(quality_colors.get(quality, '#ffffff'))\n",
    "    \n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Add a title\n",
    "    plt.suptitle('Imputation Quality Summary', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Add a legend for quality colors\n",
    "    legend_elements = [plt.Rectangle((0, 0), 1, 1, facecolor=color, edgecolor='black', \n",
    "                                   label=quality)\n",
    "                     for quality, color in quality_colors.items() \n",
    "                     if quality in metrics_df['Quality'].values]\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper right', title='Imputation Quality')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_quality_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Imputation quality summary visualization saved to 'imputation_quality_summary.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEGINNING IMPUTATION PROCESS FOR FEMALE FARMERS DATASET ===\n",
      "\n",
      "Loading data...\n",
      "Loaded 80 rows and 74 columns\n",
      "\n",
      "Analyzing missing data...\n",
      "\n",
      "Missing data summary:\n",
      "  Age ménopause: 51 missing values (63.7%)\n",
      "  Chemical: 41 missing values (51.2%)\n",
      "  Thermal: 34 missing values (42.5%)\n",
      "  Bio: 31 missing values (38.8%)\n",
      "  Nb pers à charge: 30 missing values (37.5%)\n",
      "  Niveau socio-économique: 27 missing values (33.8%)\n",
      "  Profession du mari: 23 missing values (28.7%)\n",
      "  Fertilizer: 16 missing values (20.0%)\n",
      "  Statut: 8 missing values (10.0%)\n",
      "  J travail / Sem: 4 missing values (5.0%)\n",
      "  Age: 3 missing values (3.8%)\n",
      "  H travail / jour: 3 missing values (3.8%)\n",
      "  Ancienneté agricole: 3 missing values (3.8%)\n",
      "  Neffa: 2 missing values (2.5%)\n",
      "  Tabagisme: 2 missing values (2.5%)\n",
      "  Niveau scolaire: 2 missing values (2.5%)\n",
      "  Nb enfants: 1 missing values (1.2%)\n",
      "  Fumées de Tabouna: 1 missing values (1.2%)\n",
      "  Situation maritale: 1 missing values (1.2%)\n",
      "  Transport: 1 missing values (1.2%)\n",
      "\n",
      "Applying initial fixes...\n",
      "\n",
      "Checking and fixing logical constraints...\n",
      "  Found 8 cases with unrealistic work experience\n",
      "\n",
      "Step 1: Applying conditional imputation for MNAR variables...\n",
      "\n",
      "Applying conditional imputation...\n",
      "  Conditionally imputing 2 values in Age ménopause where Ménopause=1\n",
      "    Used custom age-based approach with average difference of 10.3 years\n",
      "\n",
      "Step 2: Determining optimal k values using hierarchical clustering (CAH)...\n",
      "\n",
      "Determining optimal k for Demographic Variables using CAH...\n",
      "  Optimal k determined for Demographic Variables: 3\n",
      "\n",
      "Determining optimal k for Health Variables using CAH...\n",
      "  Optimal k determined for Health Variables: 6\n",
      "\n",
      "Determining optimal k for Work Variables using CAH...\n",
      "  Optimal k determined for Work Variables: 5\n",
      "\n",
      "Step 3: Applying specific imputation techniques based on missingness mechanisms...\n",
      "\n",
      "Imputing strong MAR variables with MICE...\n",
      "  Imputing 1 columns with MICE...\n",
      "    Imputed 30 values in Nb pers à charge\n",
      "\n",
      "Imputing Niveau socio-économique with PMM...\n",
      "  Using 67 predictor columns for PMM\n",
      "  Imputing Niveau socio-économique with PMM...\n",
      "\n",
      "Imputing MCAR binary indicators for Chemical...\n",
      "\n",
      "Imputing MCAR binary indicators for Bio...\n",
      "\n",
      "Imputing MCAR binary indicators for Fertilizer...\n",
      "\n",
      "Imputing MCAR binary indicators for Thermal...\n",
      "\n",
      "Imputing MCAR binary indicators for Transport...\n",
      "\n",
      "Imputing low missingness variables with KNN...\n",
      "  Imputing 2 columns with KNN (k=3)...\n",
      "    Imputed 3 values in Age\n",
      "    Imputed 1 values in Nb enfants\n",
      "  Imputing 3 columns with KNN (k=5)...\n",
      "    Imputed 3 values in H travail / jour\n",
      "    Imputed 3 values in Ancienneté agricole\n",
      "    Imputed 4 values in J travail / Sem\n",
      "  Imputing 3 columns with KNN (k=3)...\n",
      "    Imputed 2 values in Neffa\n",
      "    Imputed 1 values in Fumées de Tabouna\n",
      "    Imputed 2 values in Tabagisme\n",
      "  Imputing 2 columns with KNN (k=3)...\n",
      "    Imputed 1 values in Situation maritale\n",
      "    Imputed 2 values in Niveau scolaire\n",
      "\n",
      "Post-processing: Fixing data types and constraints...\n",
      "\n",
      "Checking and fixing logical constraints...\n",
      "  Found 1 cases with unrealistic work experience\n",
      "\n",
      "Validating imputation results...\n",
      "\n",
      "Validating imputation results...\n",
      "  Age: mean diff = 0.33, std diff = 0.07\n",
      "  Nb enfants: mean diff = 0.02, std diff = 0.00\n",
      "  Nb pers à charge: mean diff = 0.02, std diff = 0.29\n",
      "  H travail / jour: mean diff = 0.07, std diff = 0.01\n",
      "  Age ménopause: mean diff = 0.21, std diff = 0.70\n",
      "  Ancienneté agricole: mean diff = 3.47, std diff = 2.19\n",
      "  J travail / Sem: mean diff = 0.03, std diff = 0.01\n",
      "  Neffa: mean diff = 0.00, std diff = 0.00\n",
      "  Fumées de Tabouna: mean diff = 0.00, std diff = 0.00\n",
      "  Tabagisme: mean diff = 0.03, std diff = 0.08\n",
      "  Situation maritale: mean diff = 0.00, std diff = 0.00\n",
      "  Niveau socio-économique: mean diff = 0.03, std diff = 0.07\n",
      "  Statut: mean diff = 0.00, std diff = 0.00\n",
      "  Niveau scolaire: mean diff = 0.01, std diff = 0.01\n",
      "  Validation plots saved to 'imputation_validation.png'\n",
      "\n",
      "Saving imputed data...\n",
      "Imputed data saved to 'imputed_female_farmers_data.xlsx'\n",
      "\n",
      "=== IMPUTATION SUMMARY ===\n",
      "Total rows: 80\n",
      "Total columns: 74\n",
      "\n",
      "Warning: Some columns still have missing values:\n",
      "  Mécanisme AT: 55 missing values\n",
      "  Age ménopause: 49 missing values\n",
      "  Antécédents gynéco: 6 missing values\n",
      "  Statut: 8 missing values\n",
      "\n",
      "=== IMPUTATION PROCESS COMPLETED ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to determine optimal k using hierarchical clustering (CAH)\n",
    "def determine_optimal_k(data, columns, min_k=3, max_k=15, prefix=\"\"):\n",
    "    \"\"\"Use hierarchical clustering to determine optimal k for KNN imputation\"\"\"\n",
    "    print(f\"\\nDetermining optimal k for {prefix} using CAH...\")\n",
    "    numeric_data = data[columns].select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop columns with all NaN values\n",
    "    numeric_data = numeric_data.loc[:, numeric_data.notna().any()]\n",
    "    \n",
    "    # If no columns remain, return default k\n",
    "    if numeric_data.shape[1] == 0:\n",
    "        print(f\"  No valid numeric columns for {prefix}. Using default k=5\")\n",
    "        return 5\n",
    "    \n",
    "    # Get complete cases (rows with no NaN)\n",
    "    clean_data = numeric_data.dropna()\n",
    "    \n",
    "    if len(clean_data) < 5:\n",
    "        print(f\"  Not enough complete cases for {prefix}. Using default k=5\")\n",
    "        return 5\n",
    "        \n",
    "    try:\n",
    "        # Standardize data for clustering\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(clean_data)\n",
    "        \n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(scaled_data, method='ward')\n",
    "        \n",
    "        # Plot dendrogram\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        dendrogram(Z, truncate_mode='level', p=5)\n",
    "        plt.title(f'Hierarchical Clustering Dendrogram for {prefix}')\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'dendrogram_{prefix.lower().replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Calculate the differences between consecutive merges\n",
    "        last_merge_distances = Z[:, 2]\n",
    "        distance_diffs = np.diff(last_merge_distances)\n",
    "        \n",
    "        if len(distance_diffs) > 0:\n",
    "            # Find the point of maximum difference (elbow method)\n",
    "            max_diff_idx = np.argmax(distance_diffs)\n",
    "            optimal_k = len(clean_data) - max_diff_idx\n",
    "            \n",
    "            # Check if the differences are significant enough\n",
    "            if distance_diffs[max_diff_idx] < 0.1 * np.mean(last_merge_distances):\n",
    "                print(f\"  No clear elbow found for {prefix}. Using alternative method.\")\n",
    "                # Alternative method: find a reasonable number of clusters\n",
    "                optimal_k = min(len(clean_data) // 5, max_k)\n",
    "        else:\n",
    "            optimal_k = min(len(clean_data) // 5, max_k)\n",
    "        \n",
    "        # Constrain k to reasonable bounds\n",
    "        optimal_k = max(min_k, min(optimal_k, max_k, len(clean_data) // 3))\n",
    "        \n",
    "        print(f\"  Optimal k determined for {prefix}: {optimal_k}\")\n",
    "        return optimal_k\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error in hierarchical clustering for {prefix}: {e}\")\n",
    "        print(\"  Using default k=5\")\n",
    "        return 5\n",
    "\n",
    "# Function for Predictive Mean Matching (PMM)\n",
    "def pmm_impute(data, target_cols, predictor_cols=None, n_neighbors=5, random_state=42):\n",
    "    \"\"\"Impute values using Predictive Mean Matching\"\"\"\n",
    "    if not target_cols:\n",
    "        return data\n",
    "        \n",
    "    imputed_data = data.copy()\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # If no predictor columns are specified, use all numeric columns except target\n",
    "    if predictor_cols is None:\n",
    "        predictor_cols = [col for col in data.columns \n",
    "                        if col not in target_cols \n",
    "                        and pd.api.types.is_numeric_dtype(data[col])]\n",
    "    \n",
    "    # Drop predictor columns with more than 50% missing values\n",
    "    valid_predictors = [col for col in predictor_cols \n",
    "                       if data[col].isnull().mean() < 0.5]\n",
    "    \n",
    "    if not valid_predictors:\n",
    "        print(\"  Warning: No valid predictor columns for PMM\")\n",
    "        return imputed_data\n",
    "        \n",
    "    print(f\"  Using {len(valid_predictors)} predictor columns for PMM\")\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col not in data.columns:\n",
    "            continue\n",
    "            \n",
    "        if data[col].isnull().sum() == 0:\n",
    "            print(f\"  No missing values in {col}\")\n",
    "            continue\n",
    "            \n",
    "        if data[col].isnull().sum() == len(data):\n",
    "            print(f\"  All values missing in {col}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Imputing {col} with PMM...\")\n",
    "        missing_idx = data[col].isnull()\n",
    "        observed_idx = ~missing_idx\n",
    "        \n",
    "        observed_values = data.loc[observed_idx, col].values\n",
    "        \n",
    "        if len(observed_values) <= n_neighbors:\n",
    "            print(f\"  Too few observed values for {col}. Using random sampling.\")\n",
    "            for idx in data[missing_idx].index:\n",
    "                imputed_data.loc[idx, col] = np.random.choice(observed_values)\n",
    "            continue\n",
    "            \n",
    "        X_train = data.loc[observed_idx, valid_predictors].copy()\n",
    "        for p in valid_predictors:\n",
    "            X_train[p] = X_train[p].fillna(X_train[p].median())\n",
    "            \n",
    "        X_test = data.loc[missing_idx, valid_predictors].copy()\n",
    "        for p in valid_predictors:\n",
    "            X_test[p] = X_test[p].fillna(data[p].median())\n",
    "            \n",
    "        if len(X_train) > 0 and len(X_test) > 0:\n",
    "            try:\n",
    "                # Standardize data for better distance calculation\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Find k nearest neighbors\n",
    "                nn_model = NearestNeighbors(n_neighbors=min(n_neighbors, len(X_train)))\n",
    "                nn_model.fit(X_train_scaled)\n",
    "                distances, indices = nn_model.kneighbors(X_test_scaled)\n",
    "                \n",
    "                # Sample value from k nearest neighbors for each missing value\n",
    "                for i, idx in enumerate(data[missing_idx].index):\n",
    "                    # Sample with probability inversely proportional to distance\n",
    "                    if np.any(distances[i]):  # Check if distances array is not all zeros\n",
    "                        weights = 1 / (distances[i] + 1e-10)  # Add small constant to avoid division by zero\n",
    "                        weights = weights / np.sum(weights)  # Normalize to sum to 1\n",
    "                        neighbor_idx = np.random.choice(indices[i], p=weights)\n",
    "                    else:\n",
    "                        neighbor_idx = np.random.choice(indices[i])\n",
    "                        \n",
    "                    donor_idx = X_train.index[neighbor_idx]\n",
    "                    imputed_data.loc[idx, col] = data.loc[donor_idx, col]\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error in PMM for {col}: {e}\")\n",
    "                print(\"  Falling back to random sampling\")\n",
    "                for idx in data[missing_idx].index:\n",
    "                    imputed_data.loc[idx, col] = np.random.choice(observed_values)\n",
    "                    \n",
    "    return imputed_data\n",
    "\n",
    "# Function for MICE (Multiple Imputation by Chained Equations)\n",
    "def mice_impute(data, target_cols, categorical_cols=None, n_iter=10, random_state=42):\n",
    "    \"\"\"Impute values using Multiple Imputation by Chained Equations (MICE)\"\"\"\n",
    "    if not target_cols:\n",
    "        return data\n",
    "        \n",
    "    imputed_data = data.copy()\n",
    "    \n",
    "    # Select columns to include in MICE based on missingness and correlation\n",
    "    numeric_cols = [col for col in data.columns \n",
    "                   if pd.api.types.is_numeric_dtype(data[col])\n",
    "                   and data[col].isnull().mean() < 0.5]\n",
    "    \n",
    "    if len(numeric_cols) < 3:\n",
    "        print(\"  Not enough valid numeric columns for MICE. Using PMM instead.\")\n",
    "        return pmm_impute(data, target_cols, n_neighbors=5, random_state=random_state)\n",
    "    \n",
    "    # Create a MICE imputer with appropriate estimator\n",
    "    print(f\"  Imputing {len(target_cols)} columns with MICE...\")\n",
    "    try:\n",
    "        # BayesianRidge is robust for mixed variable types\n",
    "        mice_imputer = IterativeImputer(\n",
    "            estimator=BayesianRidge(),\n",
    "            max_iter=n_iter,\n",
    "            random_state=random_state,\n",
    "            skip_complete=True\n",
    "        )\n",
    "        \n",
    "        # Prepare data for MICE: only include numeric columns and handle categorical explicitly\n",
    "        mice_data = imputed_data[numeric_cols].copy()\n",
    "        \n",
    "        # Run MICE imputation\n",
    "        imputed_values = mice_imputer.fit_transform(mice_data)\n",
    "        \n",
    "        # Update the imputed dataframe with MICE results\n",
    "        mice_result = pd.DataFrame(imputed_values, columns=numeric_cols, index=mice_data.index)\n",
    "        \n",
    "        # Replace only the target columns to avoid modifying other columns\n",
    "        for col in target_cols:\n",
    "            if col in numeric_cols:\n",
    "                # Copy only the previously missing values from MICE results\n",
    "                missing_mask = imputed_data[col].isnull()\n",
    "                imputed_data.loc[missing_mask, col] = mice_result.loc[missing_mask, col]\n",
    "                print(f\"    Imputed {missing_mask.sum()} values in {col}\")\n",
    "                \n",
    "        # Handle categorical columns separately if specified\n",
    "        if categorical_cols:\n",
    "            cat_cols_to_impute = [col for col in categorical_cols if col in target_cols]\n",
    "            if cat_cols_to_impute:\n",
    "                print(\"  Handling categorical variables separately...\")\n",
    "                for col in cat_cols_to_impute:\n",
    "                    # Use mode imputation for categorical variables\n",
    "                    missing_mask = imputed_data[col].isnull()\n",
    "                    if missing_mask.sum() > 0:\n",
    "                        mode_val = imputed_data[col].mode().iloc[0]\n",
    "                        imputed_data.loc[missing_mask, col] = mode_val\n",
    "                        print(f\"    Imputed {missing_mask.sum()} values in {col} with mode\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error in MICE: {e}\")\n",
    "        print(\"  Falling back to PMM imputation\")\n",
    "        return pmm_impute(data, target_cols, n_neighbors=5, random_state=random_state)\n",
    "        \n",
    "    return imputed_data\n",
    "\n",
    "# Function for KNN imputation\n",
    "def knn_impute(data, target_cols, n_neighbors=5, weights='uniform'):\n",
    "    \"\"\"Impute values using KNN\"\"\"\n",
    "    if not target_cols:\n",
    "        return data\n",
    "        \n",
    "    imputed_data = data.copy()\n",
    "    \n",
    "    # Only select columns where KNN makes sense (numeric)\n",
    "    numeric_cols = [col for col in data.columns \n",
    "                  if pd.api.types.is_numeric_dtype(data[col])]\n",
    "    \n",
    "    # Prepare data for imputation\n",
    "    target_numeric = [col for col in target_cols if col in numeric_cols]\n",
    "    \n",
    "    if not target_numeric:\n",
    "        print(\"  No valid numeric columns for KNN imputation\")\n",
    "        return imputed_data\n",
    "        \n",
    "    print(f\"  Imputing {len(target_numeric)} columns with KNN (k={n_neighbors})...\")\n",
    "    \n",
    "    try:\n",
    "        # Create KNN imputer\n",
    "        knn_imputer = KNNImputer(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            metric='nan_euclidean'\n",
    "        )\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputed_values = knn_imputer.fit_transform(imputed_data[numeric_cols])\n",
    "        \n",
    "        # Update dataframe with imputed values\n",
    "        imputed_numeric = pd.DataFrame(\n",
    "            imputed_values,\n",
    "            columns=numeric_cols,\n",
    "            index=imputed_data.index\n",
    "        )\n",
    "        \n",
    "        # Copy only target columns back to main dataframe\n",
    "        for col in target_numeric:\n",
    "            # Only update missing values\n",
    "            missing_mask = imputed_data[col].isnull()\n",
    "            imputed_data.loc[missing_mask, col] = imputed_numeric.loc[missing_mask, col]\n",
    "            print(f\"    Imputed {missing_mask.sum()} values in {col}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error in KNN imputation: {e}\")\n",
    "        print(\"  Falling back to simple imputation\")\n",
    "        \n",
    "        # Simple fallback: median for numeric\n",
    "        for col in target_numeric:\n",
    "            missing_mask = imputed_data[col].isnull()\n",
    "            if missing_mask.sum() > 0:\n",
    "                median_val = imputed_data[col].median()\n",
    "                imputed_data.loc[missing_mask, col] = median_val\n",
    "                print(f\"    Imputed {missing_mask.sum()} values in {col} with median\")\n",
    "                \n",
    "    return imputed_data\n",
    "\n",
    "# Function for mode imputation\n",
    "def mode_impute(data, target_cols):\n",
    "    \"\"\"Impute categorical variables using mode\"\"\"\n",
    "    if not target_cols:\n",
    "        return data\n",
    "        \n",
    "    imputed_data = data.copy()\n",
    "    print(f\"  Imputing {len(target_cols)} columns with mode...\")\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col not in data.columns:\n",
    "            continue\n",
    "            \n",
    "        missing_mask = imputed_data[col].isnull()\n",
    "        if missing_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the mode value\n",
    "        mode_val = imputed_data[col].mode().iloc[0]\n",
    "        \n",
    "        # Apply imputation\n",
    "        imputed_data.loc[missing_mask, col] = mode_val\n",
    "        print(f\"    Imputed {missing_mask.sum()} values in {col} with mode {mode_val}\")\n",
    "        \n",
    "    return imputed_data\n",
    "\n",
    "# Function for Random Forest imputation\n",
    "def rf_impute(data, target_cols, categorical_cols=None, n_estimators=100, random_state=42):\n",
    "    \"\"\"Impute values using Random Forest\"\"\"\n",
    "    if not target_cols:\n",
    "        return data\n",
    "        \n",
    "    imputed_data = data.copy()\n",
    "    \n",
    "    # Start with median/mode imputation to get a complete dataset for RF\n",
    "    # This will be refined by the RF algorithm\n",
    "    temp_data = imputed_data.copy()\n",
    "    for col in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            temp_data[col] = temp_data[col].fillna(temp_data[col].median())\n",
    "        else:\n",
    "            temp_data[col] = temp_data[col].fillna(temp_data[col].mode().iloc[0])\n",
    "    \n",
    "    # Process each target column\n",
    "    for col in target_cols:\n",
    "        if col not in data.columns:\n",
    "            continue\n",
    "            \n",
    "        missing_mask = imputed_data[col].isnull()\n",
    "        if missing_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Imputing {col} with Random Forest...\")\n",
    "        \n",
    "        # Select features\n",
    "        features = [c for c in temp_data.columns \n",
    "                   if c != col \n",
    "                   and pd.api.types.is_numeric_dtype(temp_data[c])\n",
    "                   and temp_data[c].isnull().sum() == 0]\n",
    "        \n",
    "        if len(features) < 2:\n",
    "            print(f\"    Not enough features for {col}. Using simple imputation.\")\n",
    "            if pd.api.types.is_numeric_dtype(imputed_data[col]):\n",
    "                imputed_data.loc[missing_mask, col] = imputed_data[col].median()\n",
    "            else:\n",
    "                imputed_data.loc[missing_mask, col] = imputed_data[col].mode().iloc[0]\n",
    "            continue\n",
    "        \n",
    "        # Split into train (known values) and test (missing values)\n",
    "        X_train = temp_data.loc[~missing_mask, features]\n",
    "        y_train = temp_data.loc[~missing_mask, col]\n",
    "        X_test = temp_data.loc[missing_mask, features]\n",
    "        \n",
    "        try:\n",
    "            # Train Random Forest model (classifier for categorical, regressor for numeric)\n",
    "            if categorical_cols and col in categorical_cols:\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            else:\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "                \n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict missing values\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Update the imputed dataframe\n",
    "            imputed_data.loc[missing_mask, col] = y_pred\n",
    "            print(f\"    Imputed {missing_mask.sum()} values in {col}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error in RF imputation for {col}: {e}\")\n",
    "            print(\"    Falling back to simple imputation\")\n",
    "            if pd.api.types.is_numeric_dtype(imputed_data[col]):\n",
    "                imputed_data.loc[missing_mask, col] = imputed_data[col].median()\n",
    "            else:\n",
    "                imputed_data.loc[missing_mask, col] = imputed_data[col].mode().iloc[0]\n",
    "                \n",
    "    return imputed_data\n",
    "\n",
    "# Function for conditional imputation\n",
    "def conditional_impute(data, conditions):\n",
    "    \"\"\"Impute values based on logical conditions\"\"\"\n",
    "    imputed_data = data.copy()\n",
    "    print(\"\\nApplying conditional imputation...\")\n",
    "    \n",
    "    for condition in conditions:\n",
    "        col = condition['column']\n",
    "        filter_col = condition['filter_column']\n",
    "        filter_value = condition['filter_value']\n",
    "        strategy = condition['strategy']\n",
    "        \n",
    "        if col not in data.columns or filter_col not in data.columns:\n",
    "            print(f\"  Column {col} or {filter_col} not found in data\")\n",
    "            continue\n",
    "            \n",
    "        filter_mask = data[filter_col] == filter_value\n",
    "        missing_mask = data[col].isnull() & filter_mask\n",
    "        \n",
    "        if missing_mask.sum() == 0:\n",
    "            print(f\"  No conditional missing values in {col}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Conditionally imputing {missing_mask.sum()} values in {col} where {filter_col}={filter_value}\")\n",
    "        \n",
    "        if strategy == 'zero':\n",
    "            imputed_data.loc[missing_mask, col] = 0\n",
    "        elif strategy == 'mode':\n",
    "            subset_mode = data.loc[filter_mask & ~data[col].isnull(), col].mode()\n",
    "            if len(subset_mode) > 0:\n",
    "                imputed_data.loc[missing_mask, col] = subset_mode.iloc[0]\n",
    "            else:\n",
    "                imputed_data.loc[missing_mask, col] = data[col].mode().iloc[0]\n",
    "        elif strategy == 'median':\n",
    "            subset_median = data.loc[filter_mask & ~data[col].isnull(), col].median()\n",
    "            if not np.isnan(subset_median):\n",
    "                imputed_data.loc[missing_mask, col] = subset_median\n",
    "            else:\n",
    "                imputed_data.loc[missing_mask, col] = data[col].median()\n",
    "        elif strategy == 'custom':\n",
    "            # Handle Age ménopause specifically\n",
    "            if col == 'Age ménopause':\n",
    "                # For menopausal women: age - random(3-10)\n",
    "                subset = data.loc[filter_mask & ~data[col].isnull()]\n",
    "                if len(subset) > 0:\n",
    "                    avg_diff = (subset['Age'] - subset['Age ménopause']).mean()\n",
    "                    avg_diff = max(5, min(15, avg_diff))  # Constrain to reasonable range\n",
    "                else:\n",
    "                    avg_diff = 7  # Default assumption\n",
    "                    \n",
    "                for idx in data[missing_mask].index:\n",
    "                    age = data.loc[idx, 'Age']\n",
    "                    if not np.isnan(age):\n",
    "                        random_diff = np.random.uniform(avg_diff - 2, avg_diff + 2)\n",
    "                        imputed_data.loc[idx, col] = max(35, age - random_diff)\n",
    "                    else:\n",
    "                        imputed_data.loc[idx, col] = np.random.uniform(45, 55)  # Default range\n",
    "                        \n",
    "                print(f\"    Used custom age-based approach with average difference of {avg_diff:.1f} years\")\n",
    "        \n",
    "        # Handle the opposite condition (e.g., non-menopausal women)\n",
    "        if condition.get('handle_opposite', False):\n",
    "            opposite_mask = (data[filter_col] != filter_value) & ~data[col].isnull()\n",
    "            if opposite_mask.sum() > 0:\n",
    "                print(f\"  Setting {opposite_mask.sum()} values to NaN in {col} where {filter_col}!={filter_value}\")\n",
    "                imputed_data.loc[opposite_mask, col] = np.nan\n",
    "                \n",
    "    return imputed_data\n",
    "\n",
    "# Function to fix logical constraints\n",
    "def fix_logical_constraints(data):\n",
    "    \"\"\"Fix logical inconsistencies in the data\"\"\"\n",
    "    print(\"\\nChecking and fixing logical constraints...\")\n",
    "    fixed_data = data.copy()\n",
    "    \n",
    "    # Age ménopause <= Age for menopausal women\n",
    "    if 'Age ménopause' in fixed_data.columns and 'Age' in fixed_data.columns:\n",
    "        invalid_age = fixed_data[(fixed_data['Age ménopause'].notna()) & \n",
    "                                (fixed_data['Age'].notna()) & \n",
    "                                (fixed_data['Age ménopause'] > fixed_data['Age'])]\n",
    "        \n",
    "        if len(invalid_age) > 0:\n",
    "            print(f\"  Found {len(invalid_age)} cases where Age ménopause > Age\")\n",
    "            for idx, row in invalid_age.iterrows():\n",
    "                fixed_data.loc[idx, 'Age ménopause'] = row['Age'] - np.random.uniform(1, 5)\n",
    "                \n",
    "    # Agricultural experience should be reasonable\n",
    "    if 'Ancienneté agricole' in fixed_data.columns and 'Age' in fixed_data.columns:\n",
    "        invalid_exp = fixed_data[(fixed_data['Ancienneté agricole'].notna()) & \n",
    "                                (fixed_data['Age'].notna()) & \n",
    "                                (fixed_data['Ancienneté agricole'] > (fixed_data['Age'] - 15))]\n",
    "        \n",
    "        if len(invalid_exp) > 0:\n",
    "            print(f\"  Found {len(invalid_exp)} cases with unrealistic work experience\")\n",
    "            for idx, row in invalid_exp.iterrows():\n",
    "                max_exp = max(0, row['Age'] - 15)\n",
    "                fixed_data.loc[idx, 'Ancienneté agricole'] = np.random.uniform(1, max_exp)\n",
    "                \n",
    "    # Number of children should be realistic\n",
    "    if 'Nb enfants' in fixed_data.columns and 'Age' in fixed_data.columns:\n",
    "        invalid_children = fixed_data[(fixed_data['Nb enfants'].notna()) & \n",
    "                                     (fixed_data['Age'].notna()) & \n",
    "                                     (fixed_data['Nb enfants'] > (fixed_data['Age'] - 15) / 2)]\n",
    "        \n",
    "        if len(invalid_children) > 0:\n",
    "            print(f\"  Found {len(invalid_children)} cases with unrealistic number of children\")\n",
    "            for idx, row in invalid_children.iterrows():\n",
    "                max_children = int((row['Age'] - 15) / 2)\n",
    "                fixed_data.loc[idx, 'Nb enfants'] = np.random.randint(0, max(1, max_children + 1))\n",
    "                \n",
    "    # Work hours per day should be realistic\n",
    "    if 'H travail / jour' in fixed_data.columns:\n",
    "        too_many_hours = fixed_data[fixed_data['H travail / jour'] > 16]\n",
    "        if len(too_many_hours) > 0:\n",
    "            print(f\"  Capping {len(too_many_hours)} cases with >16 work hours per day\")\n",
    "            fixed_data.loc[fixed_data['H travail / jour'] > 16, 'H travail / jour'] = 16\n",
    "            \n",
    "    # Work days per week should be realistic\n",
    "    if 'J travail / Sem' in fixed_data.columns:\n",
    "        too_many_days = fixed_data[fixed_data['J travail / Sem'] > 7]\n",
    "        if len(too_many_days) > 0:\n",
    "            print(f\"  Capping {len(too_many_days)} cases with >7 work days per week\")\n",
    "            fixed_data.loc[fixed_data['J travail / Sem'] > 7, 'J travail / Sem'] = 7\n",
    "            \n",
    "    return fixed_data\n",
    "\n",
    "# Function to validate the imputation results\n",
    "def validate_imputation(original_data, imputed_data, key_columns):\n",
    "    \"\"\"Validate the imputation results by comparing distributions\"\"\"\n",
    "    print(\"\\nValidating imputation results...\")\n",
    "    \n",
    "    # Setup plotting\n",
    "    n_cols = len(key_columns)\n",
    "    if n_cols == 0:\n",
    "        return\n",
    "        \n",
    "    n_rows = (n_cols + 2) // 3  # Ceiling division by 3\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(18, n_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Generate comparison plots for each key column\n",
    "    for i, col in enumerate(key_columns):\n",
    "        if col not in original_data.columns or col not in imputed_data.columns:\n",
    "            continue\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get original and imputed distributions\n",
    "        orig_data = original_data[col].dropna()\n",
    "        imp_data = imputed_data[col]\n",
    "        \n",
    "        # Check if numeric or categorical\n",
    "        if pd.api.types.is_numeric_dtype(orig_data):\n",
    "            if len(orig_data) > 5:  # Only plot if we have enough data\n",
    "                sns.histplot(orig_data, color='blue', alpha=0.5, label='Original', ax=ax)\n",
    "                sns.histplot(imp_data, color='red', alpha=0.5, label='Imputed', ax=ax)\n",
    "                \n",
    "                # Add vertical lines for means\n",
    "                ax.axvline(orig_data.mean(), color='blue', linestyle='--', alpha=0.7)\n",
    "                ax.axvline(imp_data.mean(), color='red', linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Add statistics\n",
    "                orig_stats = f\"Original (n={len(orig_data)}): mean={orig_data.mean():.2f}, std={orig_data.std():.2f}\"\n",
    "                imp_stats = f\"Imputed (n={len(imp_data)}): mean={imp_data.mean():.2f}, std={imp_data.std():.2f}\"\n",
    "                ax.text(0.05, 0.95, orig_stats, transform=ax.transAxes, va='top', fontsize=9)\n",
    "                ax.text(0.05, 0.90, imp_stats, transform=ax.transAxes, va='top', fontsize=9)\n",
    "                \n",
    "                # Calculate and display KS test p-value if enough data\n",
    "                if len(orig_data) >= 5 and len(imp_data) >= 5:\n",
    "                    from scipy.stats import ks_2samp\n",
    "                    ks_stat, p_val = ks_2samp(orig_data, imp_data)\n",
    "                    ax.text(0.05, 0.85, f\"KS test p-value: {p_val:.3f}\", transform=ax.transAxes, va='top', fontsize=9)\n",
    "                    \n",
    "                # Print mean and std differences\n",
    "                mean_diff = abs(orig_data.mean() - imp_data.mean())\n",
    "                std_diff = abs(orig_data.std() - imp_data.std())\n",
    "                print(f\"  {col}: mean diff = {mean_diff:.2f}, std diff = {std_diff:.2f}\")\n",
    "                \n",
    "        else:\n",
    "            # For categorical variables, use countplot\n",
    "            orig_counts = orig_data.value_counts(normalize=True)\n",
    "            imp_counts = imp_data.value_counts(normalize=True)\n",
    "            \n",
    "            # Combine indices to ensure all categories are shown\n",
    "            all_cats = sorted(set(list(orig_counts.index) + list(imp_counts.index)))\n",
    "            \n",
    "            # Create a DataFrame for plotting\n",
    "            plot_data = pd.DataFrame({'Category': [], 'Proportion': [], 'Source': []})\n",
    "            \n",
    "            for cat in all_cats:\n",
    "                if cat in orig_counts:\n",
    "                    plot_data = plot_data.append({'Category': cat, 'Proportion': orig_counts[cat], 'Source': 'Original'}, \n",
    "                                                ignore_index=True)\n",
    "                if cat in imp_counts:\n",
    "                    plot_data = plot_data.append({'Category': cat, 'Proportion': imp_counts[cat], 'Source': 'Imputed'}, \n",
    "                                                ignore_index=True)\n",
    "                    \n",
    "            # Plot\n",
    "            sns.barplot(x='Category', y='Proportion', hue='Source', data=plot_data, ax=ax)\n",
    "            ax.set_title(f'Distribution of {col}')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Calculate Hellinger distance as a measure of distribution similarity\n",
    "            hell_dist = np.sqrt(np.sum((np.sqrt(orig_counts.reindex(all_cats).fillna(0)) - \n",
    "                                       np.sqrt(imp_counts.reindex(all_cats).fillna(0))) ** 2)) / np.sqrt(2)\n",
    "            ax.text(0.05, 0.95, f\"Hellinger distance: {hell_dist:.3f}\", transform=ax.transAxes, va='top', fontsize=9)\n",
    "            print(f\"  {col}: Hellinger distance = {hell_dist:.3f}\")\n",
    "            \n",
    "        ax.set_title(f'Distribution Comparison: {col}')\n",
    "        ax.legend()\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(key_columns), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_validation.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"  Validation plots saved to 'imputation_validation.png'\")\n",
    "\n",
    "# Main function to handle the entire imputation process\n",
    "def impute_female_farmers_data():\n",
    "    \"\"\"Main function to impute the female farmers dataset\"\"\"\n",
    "    print(\"\\n=== BEGINNING IMPUTATION PROCESS FOR FEMALE FARMERS DATASET ===\\n\")\n",
    "    \n",
    "    # 1. Load the data\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        data = pd.read_excel('encoded_female_farmers_data_no_text.xlsx')\n",
    "        print(f\"Loaded {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Define column types\n",
    "    numerical_cols = ['Age', 'Nb enfants', 'Nb pers à charge', 'H travail / jour', 'Age ménopause', \n",
    "                      'Ancienneté agricole', 'J travail / Sem', 'Poids', 'Taille', 'TAS', 'TAD', 'GAD']\n",
    "    \n",
    "    binary_cols = ['Neffa', 'Fumées de Tabouna', 'AT en milieu agricole', 'Ménopause', 'Tabagisme']\n",
    "    \n",
    "    ordinal_cols = ['Situation maritale', 'Domicile', 'Niveau socio-économique', 'Statut', 'Niveau scolaire',\n",
    "                   'Masque pour pesticides', 'Bottes', 'Gants', 'Casquette/Mdhalla', 'Manteau imperméable']\n",
    "    \n",
    "    # Check which columns actually exist in the dataset\n",
    "    numerical_cols = [col for col in numerical_cols if col in data.columns]\n",
    "    binary_cols = [col for col in binary_cols if col in data.columns]\n",
    "    ordinal_cols = [col for col in ordinal_cols if col in data.columns]\n",
    "    \n",
    "    # Identify prefix patterns for one-hot encoded and binary indicator variables\n",
    "    chemical_cols = [col for col in data.columns if col.startswith('Chemical_')]\n",
    "    bio_cols = [col for col in data.columns if col.startswith('Bio_')]\n",
    "    fertilizer_cols = [col for col in data.columns if col.startswith('Fertilizer_')]\n",
    "    thermal_cols = [col for col in data.columns if col.startswith('Thermal_')]\n",
    "    transport_cols = [col for col in data.columns if col.startswith('Transport_')]\n",
    "    \n",
    "    # Identify husband profession columns (one-hot encoded)\n",
    "    husband_cols = [col for col in data.columns if col.startswith('Profession du mari_')]\n",
    "    \n",
    "    # Group columns by prefix\n",
    "    binary_indicator_groups = {\n",
    "        'Chemical': chemical_cols,\n",
    "        'Bio': bio_cols,\n",
    "        'Fertilizer': fertilizer_cols,\n",
    "        'Thermal': thermal_cols,\n",
    "        'Transport': transport_cols\n",
    "    }\n",
    "    \n",
    "    one_hot_groups = {\n",
    "        'Profession du mari': husband_cols\n",
    "    }\n",
    "    \n",
    "    # 3. Check missing data\n",
    "    print(\"\\nAnalyzing missing data...\")\n",
    "    missing_counts = {}\n",
    "    \n",
    "    # Check numerical, binary, and ordinal columns\n",
    "    for col in numerical_cols + binary_cols + ordinal_cols:\n",
    "        missing_count = data[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_counts[col] = {\n",
    "                'count': missing_count,\n",
    "                'percentage': missing_count / len(data) * 100,\n",
    "                'type': 'direct'\n",
    "            }\n",
    "    \n",
    "    # Check binary indicator groups\n",
    "    for prefix, cols in binary_indicator_groups.items():\n",
    "        if cols:\n",
    "            missing_count = data[cols].isnull().all(axis=1).sum()\n",
    "            if missing_count > 0:\n",
    "                missing_counts[prefix] = {\n",
    "                    'count': missing_count,\n",
    "                    'percentage': missing_count / len(data) * 100,\n",
    "                    'type': 'binary_indicator'\n",
    "                }\n",
    "    \n",
    "    # Check one-hot encoded groups\n",
    "    for group, cols in one_hot_groups.items():\n",
    "        nan_col = f\"{group}_nan\"\n",
    "        if nan_col in data.columns:\n",
    "            missing_count = data[nan_col].sum()\n",
    "            if missing_count > 0:\n",
    "                missing_counts[group] = {\n",
    "                    'count': missing_count,\n",
    "                    'percentage': missing_count / len(data) * 100,\n",
    "                    'type': 'one_hot'\n",
    "                }\n",
    "    \n",
    "    # Display missing data summary\n",
    "    print(\"\\nMissing data summary:\")\n",
    "    for var, info in sorted(missing_counts.items(), key=lambda x: x[1]['percentage'], reverse=True):\n",
    "        print(f\"  {var}: {info['count']} missing values ({info['percentage']:.1f}%)\")\n",
    "    \n",
    "    # 4. Apply initial fixes and logical constraints\n",
    "    print(\"\\nApplying initial fixes...\")\n",
    "    fixed_data = fix_logical_constraints(data)\n",
    "    \n",
    "    # 5. Define imputation strategy based on missing mechanism analysis\n",
    "    # Based on the analysis in the previous step\n",
    "    \n",
    "    # 5.1 Conditional imputation first (for MNAR variables)\n",
    "    print(\"\\nStep 1: Applying conditional imputation for MNAR variables...\")\n",
    "    conditional_imputation_rules = [\n",
    "        {\n",
    "            'column': 'Age ménopause',\n",
    "            'filter_column': 'Ménopause',\n",
    "            'filter_value': 1,\n",
    "            'strategy': 'custom',\n",
    "            'handle_opposite': True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    imputed_data = conditional_impute(fixed_data, conditional_imputation_rules)\n",
    "    \n",
    "    # 6. Determine optimal k for KNN-based methods using CAH\n",
    "    print(\"\\nStep 2: Determining optimal k values using hierarchical clustering (CAH)...\")\n",
    "    \n",
    "    # Different k values for different variable groups\n",
    "    k_demographic = determine_optimal_k(imputed_data, ['Age', 'Nb enfants', 'Nb pers à charge'], \n",
    "                                      min_k=3, max_k=10, prefix=\"Demographic Variables\")\n",
    "    \n",
    "    k_health = determine_optimal_k(imputed_data, ['Poids', 'Taille', 'TAS', 'TAD', 'GAD'], \n",
    "                                 min_k=3, max_k=10, prefix=\"Health Variables\")\n",
    "    \n",
    "    k_work = determine_optimal_k(imputed_data, ['H travail / jour', 'Ancienneté agricole', 'J travail / Sem'],\n",
    "                               min_k=3, max_k=10, prefix=\"Work Variables\")\n",
    "    \n",
    "    # 7. Apply imputation techniques based on missingness mechanisms\n",
    "    print(\"\\nStep 3: Applying specific imputation techniques based on missingness mechanisms...\")\n",
    "    \n",
    "    # 7.1 Strong MAR variables (with MICE)\n",
    "    strong_mar_cols = ['Nb pers à charge']\n",
    "    strong_mar_cols = [col for col in strong_mar_cols if col in imputed_data.columns and imputed_data[col].isnull().sum() > 0]\n",
    "    \n",
    "    if strong_mar_cols:\n",
    "        print(\"\\nImputing strong MAR variables with MICE...\")\n",
    "        imputed_data = mice_impute(imputed_data, strong_mar_cols, n_iter=10)\n",
    "    \n",
    "    # 7.2 Moderate MAR variables (with PMM)\n",
    "    moderate_mar_cols = ['Niveau socio-économique', 'Profession du mari']\n",
    "    moderate_mar_cols = [col for col in moderate_mar_cols if col in imputed_data.columns and imputed_data[col].isnull().sum() > 0]\n",
    "    \n",
    "    if 'Niveau socio-économique' in moderate_mar_cols:\n",
    "        print(\"\\nImputing Niveau socio-économique with PMM...\")\n",
    "        imputed_data = pmm_impute(imputed_data, ['Niveau socio-économique'], n_neighbors=k_demographic)\n",
    "    \n",
    "    if 'Profession du mari' in moderate_mar_cols:\n",
    "        print(\"\\nImputing Profession du mari (one-hot encoded columns)...\")\n",
    "        # Use KNN for one-hot encoded husband profession\n",
    "        if 'Profession du mari_nan' in imputed_data.columns:\n",
    "            # Identify rows with missing husband profession\n",
    "            missing_husband = imputed_data['Profession du mari_nan'] == 1\n",
    "            \n",
    "            if missing_husband.sum() > 0:\n",
    "                print(f\"  Found {missing_husband.sum()} rows with missing husband profession\")\n",
    "                \n",
    "                # Use random forest to predict the most likely profession category\n",
    "                rf_features = ['Age', 'Nb enfants', 'Nb pers à charge', 'Niveau socio-économique', \n",
    "                              'Ancienneté agricole', 'Niveau scolaire']\n",
    "                rf_features = [f for f in rf_features if f in imputed_data.columns]\n",
    "                \n",
    "                # Get existing husband professions (non-nan columns)\n",
    "                husband_categories = [col for col in husband_cols if not col.endswith('_nan')]\n",
    "                \n",
    "                if husband_categories and rf_features:\n",
    "                    try:\n",
    "                        # Create temporary dataset with one profession per row\n",
    "                        temp_data = imputed_data.copy()\n",
    "                        \n",
    "                        # Fill missing values in features\n",
    "                        for feature in rf_features:\n",
    "                            if temp_data[feature].isnull().sum() > 0:\n",
    "                                if pd.api.types.is_numeric_dtype(temp_data[feature]):\n",
    "                                    temp_data[feature] = temp_data[feature].fillna(temp_data[feature].median())\n",
    "                                else:\n",
    "                                    temp_data[feature] = temp_data[feature].fillna(temp_data[feature].mode().iloc[0])\n",
    "                        \n",
    "                        # For rows with known husband profession, identify which profession\n",
    "                        known_profession = pd.Series(index=temp_data.index, dtype='object')\n",
    "                        for idx in temp_data[~missing_husband].index:\n",
    "                            for prof in husband_categories:\n",
    "                                if temp_data.loc[idx, prof] == 1:\n",
    "                                    known_profession.loc[idx] = prof\n",
    "                                    break\n",
    "                        \n",
    "                        # Remove rows with no clear profession\n",
    "                        valid_mask = known_profession.notna()\n",
    "                        \n",
    "                        if valid_mask.sum() > 0:\n",
    "                            X_train = temp_data.loc[valid_mask, rf_features]\n",
    "                            y_train = known_profession[valid_mask]\n",
    "                            \n",
    "                            # Train random forest classifier\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                            rf_model.fit(X_train, y_train)\n",
    "                            \n",
    "                            # Predict profession for missing rows\n",
    "                            X_predict = temp_data.loc[missing_husband, rf_features]\n",
    "                            predictions = rf_model.predict(X_predict)\n",
    "                            \n",
    "                            # Apply predictions\n",
    "                            for i, idx in enumerate(temp_data[missing_husband].index):\n",
    "                                pred_profession = predictions[i]\n",
    "                                # Reset all profession columns to 0\n",
    "                                for prof in husband_categories:\n",
    "                                    imputed_data.loc[idx, prof] = 0\n",
    "                                # Set predicted profession to 1\n",
    "                                imputed_data.loc[idx, pred_profession] = 1\n",
    "                                # Set nan indicator to 0\n",
    "                                imputed_data.loc[idx, 'Profession du mari_nan'] = 0\n",
    "                                \n",
    "                            print(f\"  Successfully imputed husband professions using Random Forest\")\n",
    "                        else:\n",
    "                            print(\"  No valid training data for husband profession prediction\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error in husband profession imputation: {e}\")\n",
    "                        print(\"  Using mode imputation instead\")\n",
    "                        \n",
    "                        # Fallback: use most common profession\n",
    "                        most_common_prof = None\n",
    "                        max_count = 0\n",
    "                        for prof in husband_categories:\n",
    "                            count = imputed_data[prof].sum()\n",
    "                            if count > max_count:\n",
    "                                max_count = count\n",
    "                                most_common_prof = prof\n",
    "                        \n",
    "                        if most_common_prof:\n",
    "                            for idx in imputed_data[missing_husband].index:\n",
    "                                # Reset all profession columns to 0\n",
    "                                for prof in husband_categories:\n",
    "                                    imputed_data.loc[idx, prof] = 0\n",
    "                                # Set most common profession to 1\n",
    "                                imputed_data.loc[idx, most_common_prof] = 1\n",
    "                                # Set nan indicator to 0\n",
    "                                imputed_data.loc[idx, 'Profession du mari_nan'] = 0\n",
    "    \n",
    "    # 7.3 MCAR variables (with mode/median imputation)\n",
    "    mcar_cols = []\n",
    "    for prefix, cols in binary_indicator_groups.items():\n",
    "        if cols and prefix in missing_counts and missing_counts[prefix]['type'] == 'binary_indicator':\n",
    "            mcar_cols.append(prefix)\n",
    "    \n",
    "    for prefix in mcar_cols:\n",
    "        cols = binary_indicator_groups.get(prefix, [])\n",
    "        if cols:\n",
    "            print(f\"\\nImputing MCAR binary indicators for {prefix}...\")\n",
    "            # Get rows with all indicators missing\n",
    "            all_missing = imputed_data[cols].isnull().all(axis=1)\n",
    "            \n",
    "            if all_missing.sum() > 0:\n",
    "                # For each column, calculate the proportion of 1s in non-missing data\n",
    "                proportions = {}\n",
    "                for col in cols:\n",
    "                    valid_data = imputed_data.loc[~all_missing, col].dropna()\n",
    "                    if len(valid_data) > 0:\n",
    "                        proportions[col] = valid_data.mean()\n",
    "                    else:\n",
    "                        proportions[col] = 0.5  # Default if no data\n",
    "                \n",
    "                # Impute missing indicators based on these proportions\n",
    "                for idx in imputed_data[all_missing].index:\n",
    "                    for col in cols:\n",
    "                        # Generate random value based on proportion\n",
    "                        imputed_data.loc[idx, col] = np.random.choice([0, 1], p=[1-proportions[col], proportions[col]])\n",
    "    \n",
    "    # 7.4 Low missingness variables (with KNN)\n",
    "    low_missing_cols = []\n",
    "    for col in numerical_cols + binary_cols + ordinal_cols:\n",
    "        if col in missing_counts and missing_counts[col]['percentage'] < 10 and col != 'Age ménopause':\n",
    "            low_missing_cols.append(col)\n",
    "    \n",
    "    if low_missing_cols:\n",
    "        print(\"\\nImputing low missingness variables with KNN...\")\n",
    "        \n",
    "        # Group variables by type for better imputation\n",
    "        low_missing_numerical = [col for col in low_missing_cols if col in numerical_cols]\n",
    "        low_missing_binary = [col for col in low_missing_cols if col in binary_cols]\n",
    "        low_missing_ordinal = [col for col in low_missing_cols if col in ordinal_cols]\n",
    "        \n",
    "        # Use appropriate k values for different variable types\n",
    "        if low_missing_numerical:\n",
    "            # Further split numerical variables by domain\n",
    "            demographic_vars = [col for col in low_missing_numerical if col in ['Age', 'Nb enfants']]\n",
    "            health_vars = [col for col in low_missing_numerical if col in ['Poids', 'Taille', 'TAS', 'TAD', 'GAD']]\n",
    "            work_vars = [col for col in low_missing_numerical if col in ['H travail / jour', 'Ancienneté agricole', 'J travail / Sem']]\n",
    "            \n",
    "            if demographic_vars:\n",
    "                imputed_data = knn_impute(imputed_data, demographic_vars, n_neighbors=k_demographic)\n",
    "            if health_vars:\n",
    "                imputed_data = knn_impute(imputed_data, health_vars, n_neighbors=k_health)\n",
    "            if work_vars:\n",
    "                imputed_data = knn_impute(imputed_data, work_vars, n_neighbors=k_work)\n",
    "        \n",
    "        if low_missing_binary:\n",
    "            imputed_data = knn_impute(imputed_data, low_missing_binary, n_neighbors=k_demographic)\n",
    "            # Round binary values\n",
    "            for col in low_missing_binary:\n",
    "                if col in imputed_data.columns:\n",
    "                    imputed_data[col] = imputed_data[col].round().clip(0, 1)\n",
    "        \n",
    "        if low_missing_ordinal:\n",
    "            imputed_data = knn_impute(imputed_data, low_missing_ordinal, n_neighbors=k_demographic)\n",
    "            # Round ordinal values to nearest integer\n",
    "            for col in low_missing_ordinal:\n",
    "                if col in imputed_data.columns:\n",
    "                    imputed_data[col] = imputed_data[col].round()\n",
    "    \n",
    "    # 8. Post-processing: Fix data types and constraints\n",
    "    print(\"\\nPost-processing: Fixing data types and constraints...\")\n",
    "    \n",
    "    # 8.1 Round numerical values that should be integers\n",
    "    integer_cols = ['Age', 'Nb enfants', 'Nb pers à charge', 'Age ménopause', 'J travail / Sem']\n",
    "    for col in integer_cols:\n",
    "        if col in imputed_data.columns:\n",
    "            imputed_data[col] = imputed_data[col].round()\n",
    "    \n",
    "    # 8.2 Ensure binary columns are 0 or 1\n",
    "    for col in binary_cols:\n",
    "        if col in imputed_data.columns:\n",
    "            imputed_data[col] = imputed_data[col].round().clip(0, 1)\n",
    "    \n",
    "    # 8.3 Ensure ordinal columns have valid values\n",
    "    for col in ordinal_cols:\n",
    "        if col in imputed_data.columns:\n",
    "            if col in ['Masque pour pesticides', 'Bottes', 'Gants', 'Casquette/Mdhalla', 'Manteau imperméable']:\n",
    "                # These should be 0, 1, 2, or 3\n",
    "                imputed_data[col] = imputed_data[col].round().clip(0, 3)\n",
    "            elif col == 'Situation maritale':\n",
    "                # 0=célibataire, 1=mariée, 2=divorcée, 3=veuve\n",
    "                imputed_data[col] = imputed_data[col].round().clip(0, 3)\n",
    "            elif col == 'Domicile':\n",
    "                # 0=monastir, 1=sfax, 2=mahdia\n",
    "                imputed_data[col] = imputed_data[col].round().clip(0, 2)\n",
    "            elif col == 'Niveau socio-économique':\n",
    "                # 0=bas, 1=moyen, 2=bon\n",
    "                imputed_data[col] = imputed_data[col].round().clip(0, 2)\n",
    "            elif col == 'Statut':\n",
    "                # 0=permanente, 1=saisonnière\n",
    "                imputed_data[col] = imputed_data[col].round().clip(0, 1)\n",
    "            elif col == 'Niveau scolaire':\n",
    "                # 0=analphabète, 1=primaire, 2=secondaire, 3=supérieur\n",
    "                imputed_data[col] = imputed_data[col].round().clip(0, 3)\n",
    "    \n",
    "    # 8.4 Fix one-hot encoding issues (ensure one value is 1, rest are 0)\n",
    "    for group, cols in one_hot_groups.items():\n",
    "        nan_col = f\"{group}_nan\"\n",
    "        if nan_col in cols:\n",
    "            cols.remove(nan_col)\n",
    "        \n",
    "        # Ensure each row has exactly one 1\n",
    "        for idx in imputed_data.index:\n",
    "            row_values = imputed_data.loc[idx, cols]\n",
    "            if row_values.sum() != 1:\n",
    "                # If no value is 1, set the most common value to 1\n",
    "                if row_values.sum() == 0:\n",
    "                    most_common = imputed_data[cols].sum().idxmax()\n",
    "                    imputed_data.loc[idx, cols] = 0\n",
    "                    imputed_data.loc[idx, most_common] = 1\n",
    "                # If multiple values are 1, keep only the highest value\n",
    "                elif row_values.sum() > 1:\n",
    "                    max_col = row_values.idxmax()\n",
    "                    imputed_data.loc[idx, cols] = 0\n",
    "                    imputed_data.loc[idx, max_col] = 1\n",
    "                    \n",
    "            # Ensure nan indicator is 0\n",
    "            if nan_col in imputed_data.columns:\n",
    "                imputed_data.loc[idx, nan_col] = 0\n",
    "    \n",
    "    # 8.5 Final logical constraint check\n",
    "    imputed_data = fix_logical_constraints(imputed_data)\n",
    "    \n",
    "    # In your impute_female_farmers_data function, replace the validation section (around line 650) with:\n",
    "\n",
    "    # 9. Validate imputation\n",
    "    print(\"\\nValidating imputation results...\")\n",
    "    # Select only key columns that actually had missing values for validation\n",
    "    key_columns = []\n",
    "\n",
    "    # Get columns with missing values for each column type\n",
    "    for col in numerical_cols + binary_cols + ordinal_cols:\n",
    "        if col in data.columns and data[col].isnull().sum() > 0:\n",
    "            key_columns.append(col)\n",
    "            \n",
    "    # Add one-hot encoded group variables for validation if they had missing values\n",
    "    for group, cols in one_hot_groups.items():\n",
    "        nan_col = f\"{group}_nan\"\n",
    "        if nan_col in data.columns and data[nan_col].sum() > 0:\n",
    "            key_columns.append(group)\n",
    "\n",
    "    # Call our enhanced validation function\n",
    "    validate_imputation(data, imputed_data, key_columns)\n",
    "    \n",
    "    # 10. Save imputed data\n",
    "    print(\"\\nSaving imputed data...\")\n",
    "    imputed_data.to_excel('imputed_female_farmers_data.xlsx', index=False)\n",
    "    print(\"Imputed data saved to 'imputed_female_farmers_data.xlsx'\")\n",
    "    \n",
    "    # 11. Summary of imputation\n",
    "    print(\"\\n=== IMPUTATION SUMMARY ===\")\n",
    "    print(f\"Total rows: {len(imputed_data)}\")\n",
    "    print(f\"Total columns: {len(imputed_data.columns)}\")\n",
    "    \n",
    "    # Check for any remaining missing values\n",
    "    remaining_missing = imputed_data.isnull().sum()\n",
    "    remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "    \n",
    "    if len(remaining_missing) > 0:\n",
    "        print(\"\\nWarning: Some columns still have missing values:\")\n",
    "        for col, count in remaining_missing.items():\n",
    "            print(f\"  {col}: {count} missing values\")\n",
    "    else:\n",
    "        print(\"\\nAll columns successfully imputed!\")\n",
    "        \n",
    "    print(\"\\n=== IMPUTATION PROCESS COMPLETED ===\\n\")\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "# Execute the imputation process if run directly\n",
    "if __name__ == \"__main__\":\n",
    "    impute_female_farmers_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMALE FARMERS DATA - STATUT FIELD IMPUTATION\n",
      "============================================================\n",
      "==================================================\n",
      "ANALYSIS OF THE STATUT FIELD\n",
      "==================================================\n",
      "Total rows: 80\n",
      "Statut field - Missing values: 8\n",
      "Statut field - Value counts:\n",
      "Statut\n",
      "0.0    61\n",
      "1.0    11\n",
      "NaN     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "IMPUTING THE STATUT FIELD WITH RANDOM FOREST\n",
      "==================================================\n",
      "Training Random Forest model...\n",
      "Model accuracy: 0.8667\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        13\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.43      0.50      0.46        15\n",
      "weighted avg       0.75      0.87      0.80        15\n",
      "\n",
      "\n",
      "Feature importances:\n",
      "                 Feature  Importance\n",
      "0                    Age    0.204697\n",
      "1    Ancienneté agricole    0.158840\n",
      "2       H travail / jour    0.133433\n",
      "8               Domicile    0.112464\n",
      "3        J travail / Sem    0.101972\n",
      "7       Nb pers à charge    0.088951\n",
      "6             Nb enfants    0.062787\n",
      "5     Situation maritale    0.047269\n",
      "9  AT en milieu agricole    0.046540\n",
      "4        Niveau scolaire    0.043047\n",
      "\n",
      "Imputed 8 missing values in Statut field\n",
      "New Statut distribution:\n",
      "Statut\n",
      "0.0    68\n",
      "1.0    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "VALIDATING STATUT IMPUTATION\n",
      "==================================================\n",
      "\n",
      "Chi-squared test for distribution similarity:\n",
      "Chi2 statistic: 8.4233, p-value: 0.0148\n",
      "Distributions are significantly different (p ≤ 0.05)\n",
      "\n",
      "Updated dataset saved to: imputed_female_farmers_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Set the style for visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "imputed_data_path = \"imputed_female_farmers_data.xlsx\"\n",
    "output_dir = \"./\"  # Current directory\n",
    "\n",
    "# Load the already imputed dataset\n",
    "try:\n",
    "    imputed_df = pd.read_excel(imputed_data_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {imputed_data_path} was not found. Please check the file path.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the dataset: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 1. EXPLORATORY ANALYSIS OF THE STATUT FIELD\n",
    "# ===========================================\n",
    "\n",
    "def analyze_statut_field(df):\n",
    "    \"\"\"Analyze the Statut field in the dataset\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ANALYSIS OF THE STATUT FIELD\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Total rows: {df.shape[0]}\")\n",
    "    print(f\"Statut field - Missing values: {df['Statut'].isna().sum()}\")\n",
    "    print(f\"Statut field - Value counts:\\n{df['Statut'].value_counts(dropna=False)}\")\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    ax = df['Statut'].value_counts(dropna=False).plot(kind='bar')\n",
    "    plt.title('Statut Distribution Before Imputation')\n",
    "    plt.xlabel('Statut Value')\n",
    "    plt.ylabel('Count')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(int(p.get_height())), (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_distribution_before_imputation.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 2. IMPUTE THE STATUT FIELD USING RANDOM FOREST\n",
    "# =============================================\n",
    "\n",
    "def impute_statut_field(df):\n",
    "    \"\"\"Impute the Statut field using Random Forest\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"IMPUTING THE STATUT FIELD WITH RANDOM FOREST\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a copy of the dataset\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define predictor variables based on domain knowledge\n",
    "    predictors = [\n",
    "        'Age', 'Ancienneté agricole', 'H travail / jour', 'J travail / Sem', \n",
    "        'Niveau scolaire', 'Situation maritale', 'Nb enfants', 'Nb pers à charge',\n",
    "        'Domicile', 'AT en milieu agricole'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all predictors exist in the dataset\n",
    "    predictors = [col for col in predictors if col in df.columns]\n",
    "    if not predictors:\n",
    "        print(\"No valid predictors found. Cannot impute Statut.\")\n",
    "        return df\n",
    "    \n",
    "    # Since the dataset is already imputed, we assume predictors have no missing values\n",
    "    # Check for missing values in predictors just to be safe\n",
    "    missing_predictors = df[predictors].isna().sum()\n",
    "    if missing_predictors.sum() > 0:\n",
    "        print(\"Warning: Some predictors have missing values, which should not happen in an already imputed dataset:\")\n",
    "        print(missing_predictors[missing_predictors > 0])\n",
    "        print(\"Proceeding with imputation, but results may be affected.\")\n",
    "    \n",
    "    # Get subset with non-missing Statut for training\n",
    "    train_df = df.dropna(subset=['Statut'])\n",
    "    if len(train_df) < 10:\n",
    "        print(\"Not enough non-missing Statut values to train a model. Skipping imputation.\")\n",
    "        return df\n",
    "    \n",
    "    # Split data for training and evaluation\n",
    "    X = train_df[predictors]\n",
    "    y = train_df['Statut']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a Random Forest classifier\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': predictors,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nFeature importances:\")\n",
    "    print(feature_importances)\n",
    "    \n",
    "    # Visualize feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances)\n",
    "    plt.title('Feature Importance for Statut Imputation')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_feature_importance.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Impute missing Statut values\n",
    "    missing_statut_mask = df['Statut'].isna()\n",
    "    if missing_statut_mask.sum() > 0:\n",
    "        X_missing = df.loc[missing_statut_mask, predictors]\n",
    "        predicted_statut = rf_model.predict(X_missing)\n",
    "        df.loc[missing_statut_mask, 'Statut'] = predicted_statut\n",
    "        print(f\"\\nImputed {missing_statut_mask.sum()} missing values in Statut field\")\n",
    "        print(f\"New Statut distribution:\\n{df['Statut'].value_counts(dropna=False)}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values to impute in Statut field\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. VALIDATE THE IMPUTATION\n",
    "# ==========================\n",
    "\n",
    "def validate_statut_imputation(original_df, imputed_df):\n",
    "    \"\"\"Validate the imputation of the Statut field\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"VALIDATING STATUT IMPUTATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Compare distributions\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Original distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    ax = original_df['Statut'].value_counts(dropna=False).plot(kind='bar', color='blue', alpha=0.7)\n",
    "    plt.title('Statut Distribution (Original)')\n",
    "    plt.xlabel('Statut Value')\n",
    "    plt.ylabel('Count')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(int(p.get_height())), (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    \n",
    "    # Imputed distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ax = imputed_df['Statut'].value_counts(dropna=False).plot(kind='bar', color='red', alpha=0.7)\n",
    "    plt.title('Statut Distribution (Imputed)')\n",
    "    plt.xlabel('Statut Value')\n",
    "    plt.ylabel('Count')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(int(p.get_height())), (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_distribution_comparison.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Chi-squared test to compare distributions\n",
    "    from scipy.stats import chi2_contingency\n",
    "    orig_counts = original_df['Statut'].value_counts(dropna=False).sort_index()\n",
    "    imp_counts = imputed_df['Statut'].value_counts(dropna=False).sort_index()\n",
    "    all_cats = sorted(set(orig_counts.index).union(imp_counts.index))\n",
    "    \n",
    "    contingency_table = pd.DataFrame({\n",
    "        'Original': [orig_counts.get(cat, 0) for cat in all_cats],\n",
    "        'Imputed': [imp_counts.get(cat, 0) for cat in all_cats]\n",
    "    }, index=all_cats)\n",
    "    \n",
    "    chi2, p_val, _, _ = chi2_contingency(contingency_table)\n",
    "    print(f\"\\nChi-squared test for distribution similarity:\")\n",
    "    print(f\"Chi2 statistic: {chi2:.4f}, p-value: {p_val:.4f}\")\n",
    "    if p_val > 0.05:\n",
    "        print(\"Distributions are not significantly different (p > 0.05)\")\n",
    "    else:\n",
    "        print(\"Distributions are significantly different (p ≤ 0.05)\")\n",
    "\n",
    "# 4. MAIN FUNCTION\n",
    "# ================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to impute and validate the Statut field\"\"\"\n",
    "    print(\"FEMALE FARMERS DATA - STATUT FIELD IMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Analyze the Statut field\n",
    "    analyze_statut_field(imputed_df)\n",
    "    \n",
    "    # Step 2: Impute the Statut field\n",
    "    original_df = imputed_df.copy()  # Keep a copy for validation\n",
    "    improved_df = impute_statut_field(imputed_df)\n",
    "    \n",
    "    # Step 3: Validate the imputation\n",
    "    validate_statut_imputation(original_df, improved_df)\n",
    "    \n",
    "    # Step 4: Save the updated dataset\n",
    "    try:\n",
    "        improved_df.to_excel(imputed_data_path, index=False)\n",
    "        print(f\"\\nUpdated dataset saved to: {imputed_data_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"\\nError: Could not save to {imputed_data_path}. Permission denied.\")\n",
    "        print(\"This might be because the file is open in another program (e.g., Excel) or you lack write permissions.\")\n",
    "        print(\"Trying to save to a temporary file instead...\")\n",
    "        temp_path = os.path.join(output_dir, \"imputed_female_farmers_data_temp.xlsx\")\n",
    "        try:\n",
    "            improved_df.to_excel(temp_path, index=False)\n",
    "            print(f\"Dataset saved to temporary file: {temp_path}\")\n",
    "            print(f\"Please close {imputed_data_path} in other programs and manually rename {temp_path} to {imputed_data_path}.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error: Could not save to temporary file {temp_path} either: {e2}\")\n",
    "            print(\"Please check your permissions and ensure the output directory is writable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving the dataset: {e}\")\n",
    "        print(\"Please check the file path and ensure you have write permissions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMALE FARMERS DATA - STATUT FIELD IMPUTATION\n",
      "============================================================\n",
      "Loading dataset from: imputed_female_farmers_data.xlsx\n",
      "Dataset loaded successfully: 80 rows, 74 columns\n",
      "==================================================\n",
      "ANALYSIS OF THE STATUT FIELD\n",
      "==================================================\n",
      "Total rows: 80\n",
      "Statut field - NaN values: 0\n",
      "Statut field - Empty strings: 0\n",
      "Statut field - Valid values: 80\n",
      "Statut field - Value counts:\n",
      "Statut\n",
      "0    68\n",
      "1    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "IMPUTING THE STATUT FIELD WITH RANDOM FOREST\n",
      "==================================================\n",
      "Using 12 predictor variables for imputation\n",
      "Training on 80 complete records\n",
      "Training Random Forest model...\n",
      "Model accuracy: 0.8750\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        14\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.44      0.50      0.47        16\n",
      "weighted avg       0.77      0.88      0.82        16\n",
      "\n",
      "\n",
      "Feature importances:\n",
      "                  Feature  Importance\n",
      "0                     Age    0.140082\n",
      "8                Domicile    0.130678\n",
      "2        H travail / jour    0.129284\n",
      "3         J travail / Sem    0.128954\n",
      "1     Ancienneté agricole    0.109896\n",
      "7        Nb pers à charge    0.082802\n",
      "6              Nb enfants    0.075928\n",
      "5      Situation maritale    0.059442\n",
      "11        Thermal_chaleur    0.044431\n",
      "9   AT en milieu agricole    0.041038\n",
      "4         Niveau scolaire    0.038085\n",
      "10    Chemical_pesticides    0.019379\n",
      "\n",
      "No missing values to impute in Statut field\n",
      "\n",
      "==================================================\n",
      "VALIDATING STATUT IMPUTATION\n",
      "==================================================\n",
      "\n",
      "Contingency table for Chi-square test:\n",
      "   Original  Imputed\n",
      "0        68       68\n",
      "1        12       12\n",
      "\n",
      "Chi-squared test for distribution similarity:\n",
      "Chi2 statistic: 0.0000, p-value: 1.0000\n",
      "Distributions are not significantly different (p > 0.05)\n",
      "This suggests the imputation preserved the original distribution.\n",
      "\n",
      "Error saving dataset: [Errno 13] Permission denied: 'imputed_female_farmers_data.xlsx'\n",
      "Saved to alternative file instead: improved_female_farmers_data.xlsx\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# Set the style for visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths - adjust as needed\n",
    "imputed_data_path = \"imputed_female_farmers_data.xlsx\"\n",
    "output_dir = \"./\"  # Current directory\n",
    "\n",
    "def clean_filename(varname):\n",
    "    \"\"\"Create a safe filename by removing special characters\"\"\"\n",
    "    return re.sub(r'[^\\w\\-]', '_', str(varname))\n",
    "\n",
    "def analyze_statut_field(df):\n",
    "    \"\"\"Analyze the Statut field in the dataset\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ANALYSIS OF THE STATUT FIELD\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check standard missing values\n",
    "    na_count = df['Statut'].isna().sum()\n",
    "    \n",
    "    # Check for empty strings\n",
    "    empty_string_count = (df['Statut'] == '').sum()\n",
    "    \n",
    "    # Total rows and valid values\n",
    "    total_rows = len(df)\n",
    "    valid_values = total_rows - (na_count + empty_string_count)\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Statut field - NaN values: {na_count}\")\n",
    "    print(f\"Statut field - Empty strings: {empty_string_count}\")\n",
    "    print(f\"Statut field - Valid values: {valid_values}\")\n",
    "    \n",
    "    # Get value counts including missing values\n",
    "    statut_vals = df['Statut'].copy()\n",
    "    \n",
    "    # Replace empty strings with NaN for proper value_counts\n",
    "    if empty_string_count > 0:\n",
    "        statut_vals = statut_vals.replace('', np.nan)\n",
    "    \n",
    "    print(f\"Statut field - Value counts:\\n{statut_vals.value_counts(dropna=False)}\")\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get value counts\n",
    "    counts = statut_vals.value_counts(dropna=False)\n",
    "    \n",
    "    # Create readable labels\n",
    "    labels = [str(val) if not pd.isna(val) else \"Missing\" for val in counts.index]\n",
    "    \n",
    "    # Create bar chart\n",
    "    ax = plt.bar(labels, counts.values, color='#3498db', alpha=0.7)\n",
    "    plt.title('Statut Distribution Before Imputation', fontsize=16)\n",
    "    plt.xlabel('Statut Value', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(counts.values):\n",
    "        plt.text(i, v + 0.5, str(int(v)), ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_distribution_before.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return empty_string_count > 0\n",
    "\n",
    "def impute_statut_field(df):\n",
    "    \"\"\"Impute the Statut field using Random Forest\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"IMPUTING THE STATUT FIELD WITH RANDOM FOREST\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a copy of the dataset\n",
    "    df = df.copy()\n",
    "    \n",
    "    # First, convert empty strings to NaN for proper imputation\n",
    "    if 'Statut' in df.columns:\n",
    "        empty_strings = (df['Statut'] == '').sum()\n",
    "        if empty_strings > 0:\n",
    "            print(f\"Converting {empty_strings} empty strings to NaN in Statut field\")\n",
    "            df['Statut'] = df['Statut'].replace('', np.nan)\n",
    "    \n",
    "    # Define predictor variables based on domain knowledge\n",
    "    predictors = [\n",
    "        'Age', 'Ancienneté agricole', 'H travail / jour', 'J travail / Sem', \n",
    "        'Niveau scolaire', 'Situation maritale', 'Nb enfants', 'Nb pers à charge',\n",
    "        'Domicile', 'AT en milieu agricole', 'Chemical_pesticides', 'Thermal_chaleur'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all predictors exist in the dataset\n",
    "    predictors = [col for col in predictors if col in df.columns]\n",
    "    if not predictors:\n",
    "        print(\"No valid predictors found. Cannot impute Statut.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Using {len(predictors)} predictor variables for imputation\")\n",
    "    \n",
    "    # Handle missing values in predictors\n",
    "    for col in predictors:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Filling {missing_count} missing values in predictor '{col}'\")\n",
    "            \n",
    "            # For numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            # For categorical/object columns\n",
    "            else:\n",
    "                # Get mode without NA values\n",
    "                mode_val = df[col].dropna().mode()\n",
    "                if not mode_val.empty:\n",
    "                    df[col] = df[col].fillna(mode_val[0])\n",
    "                else:\n",
    "                    print(f\"  Warning: Cannot find mode for '{col}'. Dropping from predictors.\")\n",
    "                    predictors.remove(col)\n",
    "    \n",
    "    # Get subset with non-missing Statut for training\n",
    "    train_df = df.dropna(subset=['Statut'])\n",
    "    if len(train_df) < 10:\n",
    "        print(\"Not enough non-missing Statut values to train a model.\")\n",
    "        print(\"Using mode imputation instead.\")\n",
    "        \n",
    "        # Impute with mode\n",
    "        mode_val = df['Statut'].dropna().mode()[0]\n",
    "        df['Statut'] = df['Statut'].fillna(mode_val)\n",
    "        print(f\"Imputed missing Statut values with mode: {mode_val}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    print(f\"Training on {len(train_df)} complete records\")\n",
    "    \n",
    "    # Split data for training and evaluation\n",
    "    X = train_df[predictors]\n",
    "    y = train_df['Statut']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a Random Forest classifier\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Show confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Permanente (0)', 'Saisonnière (1)'],\n",
    "                yticklabels=['Permanente (0)', 'Saisonnière (1)'])\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_confusion_matrix.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': predictors,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature importances:\")\n",
    "    print(feature_importances)\n",
    "    \n",
    "    # Visualize feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = plt.barh(feature_importances['Feature'], feature_importances['Importance'], \n",
    "                    color='#2ecc71', alpha=0.7)\n",
    "    plt.title('Feature Importance for Statut Imputation', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=14)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.01, i, f'{width:.4f}', va='center', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_feature_importance.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Impute missing Statut values\n",
    "    missing_statut_mask = df['Statut'].isna()\n",
    "    missing_count = missing_statut_mask.sum()\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(f\"\\nImputing {missing_count} missing values in Statut field\")\n",
    "        \n",
    "        # Get predictors for missing values\n",
    "        X_missing = df.loc[missing_statut_mask, predictors]\n",
    "        \n",
    "        # Predict values\n",
    "        predicted_statut = rf_model.predict(X_missing)\n",
    "        \n",
    "        # Apply predictions\n",
    "        df.loc[missing_statut_mask, 'Statut'] = predicted_statut\n",
    "        \n",
    "        # Show imputation results\n",
    "        value_counts = df['Statut'].value_counts(dropna=False)\n",
    "        print(f\"New Statut distribution:\\n{value_counts}\")\n",
    "        \n",
    "        # Show probabilities for imputed records\n",
    "        proba = rf_model.predict_proba(X_missing)\n",
    "        class_0_prob = proba[:, 0]  # Probability of class 0 (Permanente)\n",
    "        class_1_prob = proba[:, 1]  # Probability of class 1 (Saisonnière)\n",
    "        \n",
    "        print(\"\\nImputation probability summary:\")\n",
    "        print(f\"Class 0 (Permanente) - Mean: {class_0_prob.mean():.4f}, Min: {class_0_prob.min():.4f}, Max: {class_0_prob.max():.4f}\")\n",
    "        print(f\"Class 1 (Saisonnière) - Mean: {class_1_prob.mean():.4f}, Min: {class_1_prob.min():.4f}, Max: {class_1_prob.max():.4f}\")\n",
    "        \n",
    "        # Visualize the imputation probabilities\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot histogram of probabilities\n",
    "        plt.hist(class_0_prob, alpha=0.5, bins=10, label='Permanente (0)')\n",
    "        plt.hist(class_1_prob, alpha=0.5, bins=10, label='Saisonnière (1)')\n",
    "        \n",
    "        plt.title('Imputation Probabilities', fontsize=16)\n",
    "        plt.xlabel('Probability', fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'statut_imputation_probabilities.png'), dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"\\nNo missing values to impute in Statut field\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_statut_imputation(original_df, imputed_df):\n",
    "    \"\"\"Validate the imputation of the Statut field\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"VALIDATING STATUT IMPUTATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Convert empty strings to NaN for proper analysis\n",
    "    if 'Statut' in original_df.columns:\n",
    "        original_df = original_df.copy()\n",
    "        original_df['Statut'] = original_df['Statut'].replace('', np.nan)\n",
    "    \n",
    "    if 'Statut' in imputed_df.columns:\n",
    "        imputed_df = imputed_df.copy()\n",
    "        imputed_df['Statut'] = imputed_df['Statut'].replace('', np.nan)\n",
    "    \n",
    "    # Compare distributions\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Original distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Get value counts\n",
    "    orig_counts = original_df['Statut'].value_counts(dropna=False)\n",
    "    \n",
    "    # Create readable labels\n",
    "    labels = [str(val) if not pd.isna(val) else \"Missing\" for val in orig_counts.index]\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = plt.bar(labels, orig_counts.values, color='blue', alpha=0.7)\n",
    "    plt.title('Statut Distribution (Original)', fontsize=16)\n",
    "    plt.xlabel('Statut Value', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(orig_counts.values):\n",
    "        plt.text(i, v + 0.5, str(int(v)), ha='center', fontsize=12)\n",
    "    \n",
    "    # Imputed distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Get value counts\n",
    "    imp_counts = imputed_df['Statut'].value_counts(dropna=False)\n",
    "    \n",
    "    # Create readable labels\n",
    "    labels = [str(val) if not pd.isna(val) else \"Missing\" for val in imp_counts.index]\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = plt.bar(labels, imp_counts.values, color='red', alpha=0.7)\n",
    "    plt.title('Statut Distribution (Imputed)', fontsize=16)\n",
    "    plt.xlabel('Statut Value', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(imp_counts.values):\n",
    "        plt.text(i, v + 0.5, str(int(v)), ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'statut_distribution_comparison.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Run statistical tests to compare distributions\n",
    "    # First, align categories\n",
    "    all_cats = sorted(set(list(orig_counts.index) + list(imp_counts.index)))\n",
    "    # Remove NaN from all_cats (can't be compared directly)\n",
    "    all_cats = [cat for cat in all_cats if not (isinstance(cat, float) and np.isnan(cat))]\n",
    "    \n",
    "    # Create aligned frequency arrays\n",
    "    orig_freq = [orig_counts.get(cat, 0) for cat in all_cats]\n",
    "    imp_freq = [imp_counts.get(cat, 0) for cat in all_cats]\n",
    "    \n",
    "    # Try running chi-squared test\n",
    "    try:\n",
    "        from scipy.stats import chi2_contingency\n",
    "        \n",
    "        # Create contingency table\n",
    "        contingency_table = pd.DataFrame({\n",
    "            'Original': orig_freq,\n",
    "            'Imputed': imp_freq\n",
    "        }, index=all_cats)\n",
    "        \n",
    "        print(\"\\nContingency table for Chi-square test:\")\n",
    "        print(contingency_table)\n",
    "        \n",
    "        # Run chi-squared test\n",
    "        chi2, p_val, _, _ = chi2_contingency(contingency_table)\n",
    "        print(f\"\\nChi-squared test for distribution similarity:\")\n",
    "        print(f\"Chi2 statistic: {chi2:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val > 0.05:\n",
    "            print(\"Distributions are not significantly different (p > 0.05)\")\n",
    "            print(\"This suggests the imputation preserved the original distribution.\")\n",
    "        else:\n",
    "            print(\"Distributions are significantly different (p ≤ 0.05)\")\n",
    "            print(\"This suggests the imputation altered the distribution pattern.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not perform Chi-squared test: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to impute and validate the Statut field\"\"\"\n",
    "    print(\"FEMALE FARMERS DATA - STATUT FIELD IMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the imputed dataset\n",
    "    print(f\"Loading dataset from: {imputed_data_path}\")\n",
    "    try:\n",
    "        imputed_df = pd.read_excel(imputed_data_path)\n",
    "        print(f\"Dataset loaded successfully: {imputed_df.shape[0]} rows, {imputed_df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Analyze the Statut field\n",
    "    has_empty_strings = analyze_statut_field(imputed_df)\n",
    "    \n",
    "    # Step 2: Save original for comparison\n",
    "    original_df = imputed_df.copy()\n",
    "    \n",
    "    # Step 3: Impute the Statut field\n",
    "    improved_df = impute_statut_field(imputed_df)\n",
    "    \n",
    "    # Step 4: Validate the imputation\n",
    "    validate_statut_imputation(original_df, improved_df)\n",
    "    \n",
    "    # Step 5: Save the updated dataset\n",
    "    try:\n",
    "        improved_df.to_excel(imputed_data_path, index=False)\n",
    "        print(f\"\\nUpdated dataset saved to: {imputed_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving dataset: {e}\")\n",
    "        # Save to an alternative filename if there's an error\n",
    "        alt_path = \"improved_female_farmers_data.xlsx\"\n",
    "        improved_df.to_excel(alt_path, index=False)\n",
    "        print(f\"Saved to alternative file instead: {alt_path}\")\n",
    "    \n",
    "    print(\"\\nProcess completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
